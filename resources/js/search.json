[[{"l":"Bienvenido al Covenant","p":["Este repositorio hace la función de recopilar las investigaciones y documentación que se han realizado sobre el proyecto Covenant.","Sientanse libre de contribuir con el proyecto."]}],[{"l":"11/03/2025","p":["solve python version problem in local environment","set up of label studio","set up of test project","set up connection of project with aws s3 bucket"]},{"l":"Pendings","p":["Test the connection between s3 and label studio","Define type of labeling of videos","Define what to do with generated dataset"]},{"l":"12/03/2025","p":["Created another s3 bucket","Solved problems in connection from label studio to s3 bucket","Defined Source Cloud Storage and Target Cloud Storage","Defined pipeline to extract frames from video","Labeling: person detection (location and count)"]},{"i":"pendings-1","l":"Pendings","p":["Get videos to start to test the labeling","Start labeling","Upload dataset to another s3 bucket"]}],[{"l":"14/03/2025","p":["Options to make inferences of extracted frames located in the s3 bucket:","Lambda functions","Sagemaker","Lambda function seems to have a simpler and more cost effective implementation","Implemented base code for the Lambda function"]},{"l":"Todo","p":["Make function access frames in s3 bucket","Put configuration of own s3 bucket","Schedule to run every time a new frame is detected in s3 bucket","Test"]},{"l":"18/03/2025","p":["Configured lambda function in aws","Made firsts test:","Solved library errors by modifing the requirements file","Current Error","https://www.trainyolo.com/blog/deploy-yolov8-on-aws-lambda"]},{"i":"todo-1","l":"Todo","p":["Check possible causes:","Python version","Libraries version","Use SAM to implement whole procedure"]},{"l":"19/03/2025","p":["Configured SAM and reconfigured lambda function","Solved permisssion errors in AWS user (used all available for needed services, might need to leave only the must-have)","Changed python version from 3.11 to 3.9.21","Changed versions of requirements libraires","Forked repo to develop custom code for the application","Current error:","Analysis and possible solution from AmazonQ"]},{"i":"todo-2","l":"Todo:","p":["Check functionality of app.py and debug","Check possible compatibility issues with libraries","Test with test_api.py","Test using s3 images"]},{"l":"20/03/2025","p":["Solved all compatibility errors from yesterday","Changed numpy version from 2.x.x to 1.x.x","Testes with test_api.py and worked correctly"]},{"i":"todo-3","l":"Todo:","p":["Implement automatic activation of function when a new image is added in s3 bukcet","Figure out what to do with results"]},{"l":"21/03/2025","p":["Added code to handle when a new image is added to s3 bucket","Made tests by manually adding image"]},{"i":"todo-4","l":"Todo:","p":["Correrct following errors:","Possbile cause: missing permissions in lambda role","Check problem with generated output of lambda function (no extension aparently)"]},{"l":"24/03/2025","p":["Corrected permission errors","Added functionality to prediction when a new image is uploaded through workflow to s3 bucket","Tested whole workflow, worked nice"]},{"i":"todo-5","l":"Todo:","p":["Can't save results in s3 bucket when the request succeded, so check where to store them."]},{"l":"26/03/2025","p":["Added code to uplaod the predictions in a json file to a determined s3 bucket","Error log:"]},{"i":"todo-6","l":"Todo:","p":["Since aws allows only for read files, upload json format in zip package, modify it and then upload to s3 bucket","Test"]}],[{"l":"Lógica conexión cámaras con nube"},{"l":"Configuración de cámara","p":["Acceder a la interfaz web de la cámara","Asignar una ip estática","Habilitar ONVIF","Configurar credenciales","Configurar resolución, fps, bitrate, formato de compresión, etc.","Testear cámaras con ONVIF Device Manager"]},{"l":"Configuración de NVR","p":["NVR en la misma red que las cámaras","Configurar con mismas credenciales que cámaras","Usar auto-discovery para encontrar cámaras con ONVIF (agregar ip manualmente de ser necesario)"]},{"l":"Configuración de grabaciones","p":["Configurar grabación contínua o provocada por una acción(ej detección de movimiento) o cada cierto tiempo","Dividir las grabaciones en segmentos más pequeños (5 min - 10 min) con herramientas como FFmpeg."]},{"l":"Configuración de AWS","p":["Crear bucket en el servicio de S3","Configurar permisos de acceso","AWS Transfer Family soporta FTP, FTPS y SFTP para integrar directamente con s3","Configurar servidor FTP con AWS Transfer Family","Guardar archivos en bucked de AWS"]},{"l":"Configuración de NVR - AWS","p":["Acceder a interfaz administrativa de NVR","Ingresar dirección del servidor FTP proveída por AWS Transfer Family (si disponible)","Configurar script o AWS CLI en dispositivo externo para hacer la carga","Ingresar credenciales para autenticación","Específicar directorio destino dependiendo de la estructura","Testear conexiones"]},{"l":"Conexión con Label Studio","p":["Llamar a una función (ej AWS Lambda) cuando un video se suba","Generar URL para cada archivo de video","En la llamada a la función hacer un HTTP POST a Label Studio API para crear una tarea","Incluir URL, metadata"]},{"l":"Configuración de Label Studio","p":["Configurar el projecto para manejar videos y definir ripo de etiquetado","Unir resultado (ej archivo JSON) al dataset"]},{"l":"Conceptos"},{"l":"RTSP","p":["Real Time Streaming Protocol, protocolo de control de un servidor de transmisión de medios (video) de manera remota. Se usa para establecer y controlar sesiones de medios entre puntos finales. Los clientes de los servidores de medios emiten comandos como * reproducir, grabar * y pausar, para facilitar el control en tiempo real de la transmisión desde el servidor a un cliente."]},{"l":"ONVIF","p":["Open Network Video Interface Forum, estándar de industria que especifica interfaces comunes para productos de seguridad basadas en ip, como cámaras"]},{"l":"NVR","p":["Network Video Recorder, dispositivo especializado diseñado para recibir, grabar y manejar transmisiones de video digitales desde cámaras IP, el NVR graba el video que ya está en formato digital, normalmente transmitido sobre Ethernet.","Pueden recibir captura de video directamente de cámaras IP, usan cables de conexión estándar, soportan alta resolución, acceso remoto e integración con softwares de manejo de video"]},{"l":"FTP Upload","p":["File Transfer Protocol, protocolo que permite transferir archivos directamente de un dispositivo a otro, las conexiones tienen una relación de cliente y servidor, en el servidor se aloja el contenido y luego te conectas a él como cliente. Los datos se envían a través de los puertos 20 y 21"]},{"l":"HTTP Upload","p":["Usa el Hypertext Transfer Protocol, normalmente en métodos como HTTP POST para mandar archivos a un servidor web o servicio en la nube, puede simplificar la integración con APIs basadas en web y servicios."]},{"l":"Fuentes","p":["https://www.dvraid.com/guide/network-video-recorder-complete-setup-guide/#:~:text=Here%E2%80%99s%20a%20simple%20step-by-step%20process%3A%201%201.Unbox%20the,This%20allows%20you%20to%20access%20the%20NVR%20remotely.","https://chatgpt.com/share/67c8574d-e450-800c-837a-db9e1f3935ef","https://www.videoexpertsgroup.com/glossary/how-to-connect-camera-to-onvif#:~:text=How%20to%20Connect%20an%20IP%20Camera%20via%20ONVIF%3A,Step%205%3A%20Test%20and%20Monitor%20the%20Connection%20","https://aws.amazon.com/es/blogs/storage/collecting-archiving-and-retrieving-surveillance-footage-with-aws/","https://aws.amazon.com/es/blogs/iot/build-a-cloud-gateway-to-ingest-rtsp-video-to-amazon-kinesis-video-streams/","http://labelstud.io.s3-website-us-east-1.amazonaws.com/guide/storage.html#Amazon-S3"]}],[{"l":"Datasets","p":["Entrenamos un modelo de YOLO para encontrar objetos comunes en la construcción"]}],[{"l":"Avances"},{"i":"reporte-de-avances-dia-con-dia-del-etiquetadoultimo-avance","l":"Reporte de avances dia con dia del etiquetado (Ultimo Avance)"},{"l":"07/03/2025","p":["En dataset","Revisadas","Rechazadas","Sin revisar","Sin etiquetar","9984","0"]}],[{"l":"El Dron","p":["El dron actualmente está equipado con un autopilot de la marca holybro y con el firmware PX4"]},{"l":"Partes","p":["Parte","Descripción","FMU","Pixhawk 6x","Autopilot","Firmware PX4 V1.14"]}],[{"l":"Reports","p":["89# Pruebas de vuelo","Registro de las pruebas de vuelo que se realizan cada semana"]},{"l":"06 - Mar - 2025"},{"l":"13 - Mar - 2025","p":["@jeduardofr: Build para la AppImage de la version modificada de QGC aunque no se probo debido a errores de dependencias","@jeduardofr: Probar vuelo remoto con mavproxy con dron anclado al suelo, salieron varios detalles como que el dron parecia \"perder\" el control y se aceleraba por su cuenta sin recibir los comandos. La teoria esta en que es debido a como esta anclado el suelo y como el sistema de control reacciona. Sigue pendiente hacer la prueba con el dron ya en vuelo normal.","@fairbrook: Hacer prueba con video con un servidor de video remoto. La prueba fue exitosa. La dirección del servidor es: http://159.54.131.60:8889/comma/ para webrtc y rtsp://159.54.131.60:8554/comma/ para rtsp (util en qground)!!! Estas direcciones van a cambiar en el futuro !!!","@fairbrook y @jeduardofr: Validación de connexión mediante red celular. La comma no detectaba la tarjeta SIM, el modem no detectaba la SIM. El problema fue que el puerto SIM estaba dañado, al revisar a detalle, uno de los pines del puerto estaba doblado y no hacía contacto correctamente. Por lo que la solución fue doblar de vuelta a posición el pin y se corregieron los problemas. Al finalizar la prueba fue exitosa. Comando útiles para el módulo SIM:"]},{"l":"20 - Mar - 2025","p":["Este día fue mucho trabajo en equipo para hacer la prueba de vuelo con el dron anclado al suelo.","Configuración de sistema de poleas y cuerdas para mantener al dron dento de un rango de movimiento seguro","Prueba de vuelo exitosa con el sistema de poleas para prevenir caidas del dron al activar el \"kill switch\"","Prueba de vuelo exitosa con el sistema de cuerdas para prevenir colisiones con paredes, objetos y personas","Prueba de vuelo exitosa con el control remoto. Utilizando dos equipos(mavproxy y QGC) y mando de xbox. El fallo de la semana anterior fue que el dron no tenía espacio de movimiento en el eje Z"]},{"l":"24 - Mar - 2025","p":["Plan de conexión para la cámara FPV. El diagrma a continuación ejemplifica las interacciones entre los distintos componentees del sistema que permitiría la visualización de la cámara Moonlight en QGC y de forma remota en cualquier dispositivo"]},{"l":"25 - Mar -2025","p":["@fairbrook: Hacer prueba con video con un servidor de video remoto. La prueba fue exitosa. Uitilizando OBS y WHIP se pudo hacer una transmisión de video con alta calidad y latencia muy baja ~ 330ms. Al intentar hacer la transmisión de video utilizando el kit de desarrollo Jetson Nano, la calidad fue menor y con una latencia de ~ 1.3s. Por lo que se busca desarrollar un programa que permita la transmisión de video en la paltaforma Jetson Nano al utilizar \"Hardware Encoding\" y WHIP"]},{"l":"01 - Abr - 2025","p":["@fairbrook: Pruebas de latencia con video receptor y transmisor montado en dron y en estación de trabajo. Latencia aproximada de 800ms@jeduardofr: Pruebas de vuelo remoto usando puro video (sin observar el dron) con version modificada de joystick (yaw, pitch & roll factors)"]},{"l":"12 - Abr - 2025","p":["@jeduardofr: Configuracion del sensor optico para mejor lectura de distancia vertical (altura). Mantenimiento general del dron."]},{"l":"08 - May - 2025","p":["@fairbrook: Configuración de antenas de telemetría P900"]},{"l":"09 - May - 2025","p":["@fairbrook: pruebas de vuelo y recalibracion del dron"]},{"l":"12 - May - 2025","p":["@fairbrook: Recorrido por c5 Jalisco"]},{"l":"15 - May - 2025","p":["@fairbrook: Montar y configurar cámaras para el laboratorios de drones"]},{"l":"29 - May - 2025","p":["@jeduardofr y @fairbrook: Debuggear posibles errores del driver de linux para el Modulo Sierra EM7455. Pruebas de vuelo con nueva version del sistema de refrigeracion, siguientes mejoras seria dejar los puertos usb mas expuestos para facilitar su uso. A"]},{"l":"19 - Jun - 2025","p":["@Fairbrook y @jeduardofr: Debuggear posibles errores con la camara Intel Realsense D435I en la Jetson. Se intento compilando distintas versiones (2.51.1, beta mas reciente a la fecha) y ninguna termino de funcionar. Lo ultimo que se intento fue alrededor de las reglas de udev. Se reviso en windows y se actualizo el firmware de la camara (aunque no hacia falta) y todo funcionaba bien con la camara."]},{"l":"20 - Jun - 2025","p":["@Fairbrook y @jeduardofr: Instalacion de cable de ethernet para laboratorio de covenant. Debuggear camara zed con la Jetson Orin Nano. Definir prioridades para el siguiente viernes."]},{"l":"26 - Jun - 2025","p":["@Fairbrook y @jeduardofr: Debuggear conexion de jetson con pixhawk (sin mucho exito).@RodriSebas y @jeduardofr: Montar jetson junta con la camara zed (con exito)."]},{"l":"03 - Jul - 2025","p":["@Fairbrook: Creación de contenedores con Isaac sim"]},{"l":"04 - Jul - 2025","p":["@jeduardofr: Configuracion de red con Zerotier, comenzar migracion de isaac sim/pegasus 4.2 a 4.5."]},{"l":"07 - Ago - 2025","p":["@jeduardofr: Pruebas DJI Dock, apoyar a Rodrigo con lerobot Setup en Gaia."]},{"l":"14 - Ago - 2025","p":["@fairbrook: Pruebas de don DJI MAVIC 400. corrección de error de cámara P1 app actualizar firmware@jeduardofr: Pruebas de vuelo con DJI DOCK 2"]},{"l":"21 - Ago - 2025","p":["@jeduardofr: Pruebas de vuelo con DJI Dock 3@fairbrook: control del matice 4td con control"]},{"l":"26 - Ago - 2025","p":["@jeduardofr: Capacitación de DJI Dock 3 y Matrice 400"]},{"l":"27 - Ago - 2025","p":["@jeduardofr: Capacitación de DJI Terra"]},{"l":"28 - Ago - 2025","p":["@jeduardofr y @Fairbrook: Pruebas de PWM con servo externo exitosas.@jeduardofr: Demo para estudiantes del TEC de forma remota con DJI Flighthub II."]},{"l":"04 - Sep - 2025","p":["@jeduardofr y @Fairbrook: Instalacion de Gaia con Proxmox"]},{"l":"05 - SEP - 2025","p":["revisión y envío de TD P"]},{"l":"11 - SEP - 2025","p":["GPU passthrough en proxmox"]},{"l":"18 - SEP - 2025","p":["Estación del dron"]},{"l":"24 - SEP - 2025","p":["@Manuelo247: Se estuvo en la oficina. Se ayudo a Rodrigo en el inventariado y la base de datos."]},{"l":"26 - SEP - 2025","p":["@jeduardofr, @Fairbrook y @Manuelo247: pruebas con cámara Zenmuse L2. pruebas de vuelo"]},{"l":"01 - OCT - 2025","p":["@jeduardofr: Configurar D-RTK 3 con Matrice 400, misiones de vuelo para mapear el Tec."]},{"l":"02 - OCT - 2025","p":["@jeduardofr: Procesamiento de misiones con LiDAR en DJI Terra. Pruebas con Reall3d Viewer."]},{"l":"03 - OCT - 2025","p":["@jeduardofr y fairbrook: Levantamiento del Tec con coleccion Oblicua."]},{"l":"07 - OCT - 2025","p":["@jeduardofr: Pruebas con DJI Dock III y FlightHub 2","@fairbrook: Pruebas con telemetría de estación"]},{"l":"08 - OCT - 2025","p":["@jeduardofr: Pruebas con DJI Terra, QGIS & more flight missions.@fairbrook: Ayuda a armado de estación"]},{"l":"09 - OCT - 2025","p":["@jeduardofr: Mapeo de Torre al lado de BBVA (parte superior) y procesamiento.@fairbrook: Automatización de estación del dron"]},{"l":"14 - OCT - 2025","p":["@jeduardofr: Pruebas de Volumen con Analyzer de FlightHub II y de Change Detection."]},{"l":"15 - OCT - 2025","p":["@fairbrook: diagnóstico y solución de torre de aterrizaje para dron x650"]},{"l":"16 - OCT - 2025","p":["@jeduardofr: Mas pruebas con Dock III, Terra, CloudCompare y comparativa con vuelo de Matrice 400 con LiDAR vs Matrice 4TD con photo@jeduardofr: Pruebas con Terra y diferentes parametros"]},{"l":"17 - OCT - 2025","p":["@fairbrook, @Manuelo247: Pruebas con dron x560@jeduardofr: Conexion Matrice 400 con FlightHub, no se logro mandar misiones desde FH y quedamos pendiente de respuesta del equipo Aero."]},{"l":"20 - OCT - 2025","p":["@jeduardofr, @Fairbrook, @Manuelo247, @RodriSebas: Lograr volar nuestro dron x650 con zed y modulo 4G"]},{"l":"21 - OCT - 2025","p":["@fairbrook, @Manuelo247: mapping zed test"]},{"l":"23 - OCT - 2025","p":["@fairbrook, @jeduardofr: Pruebas para IMAV"]},{"l":"24 - OCT - 2025","p":["@jeduardofr: Pruebas para IMAV"]},{"l":"27 - OCT - 2025","p":["@Fairbrook: Pruebas para IMAV"]},{"l":"29 - OCT - 2025","p":["@jeduardofr y fairbrook: Pruebas mecanismo agua"]},{"l":"30 - OCT - 2025","p":["fairbrook y @jeduardofr: Pruebas exitosas de dron"]},{"l":"31 - OCT - 2025","p":["@fairbrook y @jeduardofr: Pruebas RTK y mapping para IMAV 2025"]},{"l":"01 - NOV - 2025","p":["@jeduardofr: Pruebas para IMAV"]},{"l":"03 - NOV - 2025","p":["@jeduardofr, @Manuelo247: Pruebas para IMAV en Puebla"]},{"l":"04 - NOV - 2025","p":["@Manuelo247: Pruebas outdoor de competencia IMAV"]},{"l":"05 - NOV - 2025","p":["@Manuelo247, @jeduardofr: Demo de indoor en IMAV puebla"]},{"l":"06 - NOV - 2025","p":["@Manuelo247: 4to y ultimo dia de la competencia"]},{"l":"07 - NOV - 2025","p":["@jeduardofr: Vuelo para Comma Con"]},{"l":"10 - NOV - 2025","p":["@jeduardofr: Compilar forerunner2 en mac"]},{"l":"12 - NOV - 2025","p":["@fairbrook Nimbus from Drone forge testing"]},{"l":"18 - NOV - 2025","p":["@jeduardofr: Configuracion basica de Dream Machine, UNAS Pro y U7 Pro Max Access Point de Ubiquiti."]},{"l":"21 - NOV - 2025","p":["@fairbrook: Pruebas drone forge"]},{"l":"24 - NOV - 2025","p":["@jeduardofr: Planeacion MVP de estacion y pruebas con mas equipo de Ubiquiti@Fairbrook: Revisión de PR y documentación de frigate"]},{"l":"25- NOV - 2025","p":["@fairbrook: Revisión PR de manu"]}],[{"l":"IMAV 2025","p":["The IMAV is an annual international event bringing together scientists, technologists, and enthusiasts on the research and technological development of Micro-Air Vehicles (MAVs).","The 16th IMAV 2025 will be held in Mexico, in the City of San Andres Cholula, in the State of Puebla, between the 3rd to the 7th of November, 2025.","The Mexican Robotics Federation will run the organization in collaboration with Mexican academic institutions: The Instituto Nacional de Astrofísica Óptica y Electrónica (INAOE); the Universidad de las Américas Puebla (UDLAP), the Benemerita Universidad Autonoma de Puebla (BUAP) and Centro de Investigación y de Estudios Avanzados del Instituto Politécnico Nacional (Cinvestav), Unidad Zacatenco."]},{"l":"Useful links","p":["Rule book","Inscription Form","Landing Page","TDP (WIP)"]},{"l":"What are we using?"},{"l":"Indoor","p":["Battery","Camera","Companion","Component","Controller","Datasheet","ESC","Extras","Firmware","Frame","GNSS/RTK","Motor","Part","Power distribution","Propellers","Radio","Radio Controller","Store","TBD","Telemetry","Video TX & RX"]},{"l":"Outdoor","p":["-","Battery","Camera","Companion","Component","Controller","Datasheet","ESC","Firmware","Frame","Gemfan 1555 Carbon Fiber","GNSS/RTK","H-RTK F9P","Helical Onboard Base","Holybro","Holybro docs","Holybro PDB Holybro PM06D","Holybro PDB01-V1.2 & PM06D","Microhard Telemetry Radio","Motor","Part","Pixhawk 6x","Power distribution","Propellers","PX4 Docs","PX4 v1.14","Radio","Radio Controller","Radiomaster","Remember there is CCW & CW","Store","TBD","Tekko32 F4 45A","Telemetry","Tmotor MN4014 KV330","TX16S Mark II","User manual","Video TX & RX","X650"]}],[{"l":"Find a person","p":["Mission 2: Find a person In this mission, a textual description of a person will be provided to the team. This text will describe a person who will be fully visible and located somewhere in the flight area. The description will include the GPS position where the person is likely to be. The following are examples of textual descriptions:","\"Find the person with an orange jacket and a yellow construction helmet\".","\"On the grass, there will be a person with red trousers, with glasses, and holding a suitcase\".","\"There will be a person sitting on a bench holding an umbrella with a computer next to her\".","Points will be given if:","The MAV arrives at the GPS target location autonomously.","The MAV finds the person described in the textual description. As evidence of this, the drone has to land autonomously near the person within a radius of 2 m."]},{"l":"22/08/2025","p":["Se dejo el ambiente preparado para iniciar las simulaciones de busqueda de personas. Se inicio en la investigacion para publicar puntos con mavlink y que el dron se diriga a ellos (Victor lo utilizara, podria ser util para otros retos)"]},{"l":"29/09/2025","p":["Investigacion de MAVLINK mission para misiones con movimientos mas suaves y preparacion de base de datos para invetariado"]}],[{"l":"Transporting challengue","p":["Reto asignado a Manuel"]},{"l":"Mission 3: Transportation Challenge","p":["In this challenge, teams are tasked with developing an autonomous multi-MAV system capable of collaboratively transporting an object. The mission requirements are as follows:","Each team must deploy a minimum of three MAVs to perform the transport task.","Teams may use additional MAVs, and bonus points will be awarded for doing so.","The object to be transported is a 3 kg dummy, simulating the weight and dimensions of a baby.","All MAVs must take off and land autonomously.","MAVs must depart from a designated take-off zone and navigate to a target destination, which will be defined by a randomly selected GPS coordinate within the flight area.","The object must be carried using a net, suspended cooperatively by the MAVs.","The net radius must not exceed 1 m., ensuring a coordinated transport approach.","Teams may manually place the dummy into the net before take-off, but all MAV motors must be powered off during this loading procedure. The mission will be invalidated if the dummy falls out of the net at any point during flight or landing.","Upon reaching the target location, MAVs must autonomously land within the designated area within a radius of 2 m."]},{"l":"Notes:","p":["Teams must bring their net made of any material suitable for the task.","In the Free-Fall Rescue Challenge, teams must develop an autonomous multi-MAV system capable of detecting and rescuing a falling object in midair."]},{"l":"18/08/2025","p":["La idea inicial consiste en desarrollar un sistema multi-dron con la siguiente dinámica:","Habrá un dron líder (main dron) identificado mediante un marcador ArUco.","Los drones seguidores deberán detectar este marcador y mantener una formación, siguiendo al líder a una distancia definida.","El objetivo principal es coordinar el movimiento de los drones para que se mantengan organizados y dependientes de la posición del líder."]},{"l":"21/08/2025","p":["Se ha decidido que el reto no se continuara hasta nuevo aviso"]}],[{"l":"Kevin Research"},{"l":"Controller","p":["Advantages","Ardupilot & ROS","Ardupilot, ROS, Debian, Cloud9","ARK Just a Jetson","ARK Pi6X Flow","arkelectronics","arkelectronics arkelectronics + jetson carrier","ARKV6X","Beale Bone Blue","Betaflight","Cannot change each component independently Made for specific drone Kits Short flight time No easy way of adding a companion computer No redundant sensors Need to include additional computer for onboard processing","Compatibility with Pixhawk hardware and peripherals Highly flexible configuration Multiple general ports Easily addition of companion computer Redundant sensors Multiple vendors","Compatibility with Pixhawk hardware and peripherals Multiple sensors Higghly flexible configuration Multiple general ports Jetson compatible board for both controller and Jetson Redundant sensors Single vendor complete ecosystem","Cube orange","Disadvantages","Expensive Require external sensors and GNSS","Fewer extra serial ports Requires a raspberry to work","Firmware","GNSS Included Redundant power supply inputs Redundant sensors Versatile hat platform Very active community Full raspberry pi","Greate opensource community Multiple firmware compatibility Multiple actuator outputs Included charger onboard GGrapichs accelerator, FPU and A8 CPU","High Compute capacity Included video encoders and decoders Multiple sensors and specialized connectors Multiple general purpose connectors Tight integration with companion computer Single vendor ecosystem Included optical flow","High Compute capacity Support for nvidia tooling and software Single computer for control and processing","Integrates ELRS, VTX, ESC and sensors in one board Minimal Configuration","irlock Aliexpress mini Aliexpress GPS nwblue","Jetpack","Matrix 1S Brushless Flight Controller (5IN1)","Mercado libre AM3D","Navio 2","Need to add external transmitters No esc included Need extra computer for full autonomy","Needed to add external transmitters (VTX, telemetry, etc) No ESC included Need to include additional computer for onboard processing","Newark","Not specifically designed for drones Support for only 2-cell batteries Few specialized connections","PX4","PX4 & Ardupilot","Reduced drone ports Reduced sensors Less specialized inputs High power consumption","Store"]},{"l":"Summary","p":["I would suggest the ARK Pi6x since it uses the already familiar PX4 platform and gives similar IO and sensor to the Pixhawk with the addition of a raspberry processing and memory capabilities, being the balance between compute power and light weight. The only drawback is the store as it is currently unstable and is uncertain if the controller would arrive on time.","A more cautious approach would be the Navio 2 as it still uses a raspberry but it is easier to get one. This time the drawback would be that is only the raspberry the one doing the control instead of the dedicated controller in the Pi6x"]},{"l":"Kits"},{"l":"3DR Quad zero kit","p":["The 3DR Quad Zero drone is a sub-250g focused on ultralight payloads and extended flight times (1hr). This kit is perfect as a development platform, educational or even a light show drone We developed a telemetry radio around this platform to enhance its \"developer platform\" character, as it provides the hardware to include external sensors and send information over custom MAVLink messages, or on the contrary use those messages to push commands to any actuator without disrupting the flight stack or the autopilot's pinout."]},{"l":"Frames"},{"l":"Kit Chasis F450","p":["El marco principal es de fibra de vidrio, mientras que los brazos están fabricados con nylon de poliamida ultra resistente. Es una excelente opción para tomar video aérea o FPV volar sin el uso de los soportes de montaje adicionales.","• Cuenta con 4 brazos (2 blancos y 2 rojos) que son excelentes para la orientación ya que ayuda a mantener volando en la dirección correcta sin necesidad de diferentes hélices de colores• También cuenta con conexiones de PCB integradas para la soldadura directa de sus ESC esto elimina la necesidad de una placa de distribución de energía o multicontactos desordenados manteniendo el diseño de su electrónica muy ordenada• Este chasis V3 viene con brazos moldeados más fuertes que el V1 y V2","CARACTERÍSTICAS• Material: Fibra de carbono de 2 mm y plástico• Distancia entre ejes: 450 mm• Dimensiones: 500 x 500 x 40 mm• Peso: 265 g"]}],[{"l":"Reports"},{"l":"03 - 11 - 2025","p":["Pruebas IMAV"]},{"l":"04 - 11 - 2025","p":["Competencia IMAV 2025"]},{"l":"05 - 11 - 2025","p":["Observación indoor"]},{"l":"06 - 11 - 2025","p":["Regreso a GDL IMAV 2025"]}],[{"l":"02/05/2025","p":["@VicmanGT","Now the program starts recording a video when a right face profile is detected","Once the face is not detected or isn't in that position, a timer starts to check the time since the last detection","if 10 secs have transcurred and the face is no detected the video is saved with a time stamt in .mp4 format","if the face is again detected the recording continues","this way, there are no micro videos of a couple frames because the model fails the prediction"]},{"l":"Todo:","p":["Clone code in Zeus to try in its webcam","Implement sound alarm when a right profile is detected"]},{"l":"Activity Report - 02/05/2025","p":["Email: brandon@nuclea.solutions"]},{"l":"Main Updates","p":["An RTSP service was configured so that the webcam from Nuclea's office could be accessed via RTSP streaming. image","A separate Docker container was used and configured to manage the VST more effectively.","The streaming from 4 Fimex cameras was successfully added directly to the VST UI for further analysis. image image","Investigate why the RTSP stream from the webcam did not work within the VST.","Check why I haven't been able to draw ROI zones and Tripwires on the Fimex streams that appear."]},{"l":"Activity Report - 05/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-1","l":"Main Updates","p":["Several cameras were configured within the VST (Video Stream Toolkit) in order to enable real-time streaming visualization from the Fimex system. This setup allows for continuous monitoring of the video feeds through the VST interface.","Some modifications were made to the existing script to enable local recording of the video streams from the Fimex cameras. The purpose of this change was to improve the recording quality and minimize frame loss, which had been an issue in previous versions of the setup.","An attempt was made to add ROI (Region of Interest) and tripwire tracking to all camera streams using NVIDIA's VST container. However, during testing it was discovered that errors persist in the ROI and tripwire processing logic, preventing the system from correctly handling and analyzing the drawn areas. Further investigation and debugging are required to resolve these issues and ensure accurate event detection.","A new VNC service was created so that we now have two different accesses to the UI of the Zeus computer, with the objective that Victor and I can work on different desktops, so to speak, and avoid conflicts from working at the same time or moving or closing each other's things. image"]},{"l":"05/05/2025","p":["@VicmanGT","Implemented sound based alarm that sound when a person's right side face is in front of the camera","https://github.com/user-attachments/assets/7112efbc-12bf-48ab-8224-840be2bcf940","Created a requirements.txt to implement code in Zeus server","Cloned code in Zeus server","When tried to run code got following error:","ALSA lib pulse.c:242:(pulse_connect) PulseAudio: Unable to connect: Connection refused","Possible causes are lack of permissions for the user to access the audio in the computer or lack of sound drivers or devices in it"]},{"l":"Activity Report - 06/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-2","l":"Main Updates","p":["A tunnel was successfully created using RTSP and Ngrok to add the office webcam to the VST. It was configured correctly, although for some unknown reason, the webcam image hasn't been successfully displayed within Nvidia's VST.","Tests were conducted with the Video Wall and Recorded Streams modules to record certain portions of the stream at a specific, configured schedule. image image","There are plans to use the Wowza platform to access public RTSP services with higher quality and test them in the VST for ROI tracking and tripwire analysis.","There is suspicion that the poor quality of the streams from Fimex's cameras may be due to the VPN server having very low hardware specs. I contacted Fernando (head of IT at Fimex) to look for a solution to this problem directly from their side.tection."]},{"l":"06/05/2025","p":["@VicmanGT","Investigated causes of the error from yesterday","pulseaudio is a general purpose sound server to communicate the software and the hardware, and it's not intalled","Got following error while trying to install it:","A possible solution is to use Pipewire instead, which is basically the same but more recent","Also check if there's an actuall sound device or sound card in the server computer","Also got following video error while ignoring the audio lines in the script:","This error occurs when the index in cv2.VideoCapture(index) surpases the number of available cameras in the device","Tried to access the camera via VCL -> Media -> Open capture device, on both video0 and video1","Check connection with the webcam and Zeus server"]},{"l":"07/05/2025","p":["@VicmanGT","Started with the function to detect a movement of the camera","The idea of keeping track of an object is possibly not going to work as expected","If for some reason, the detected object is moved or the model confuses the object or stop detecting it there could be a false positive","The best solution at the moment is the direct comparation of two frames","Substract the values of the pixel of each frame in grayscale","If the result if bigger that a certain threshold so the camera has been moved and an alert is generated","More info:","https://www.hackersrealm.net/post/motion-detection-tutorial-using-opencv"]},{"l":"Activity Report - 07/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-3","l":"Main Updates","p":["A meeting was scheduled with Fernando (IT manager at Fimex) to upgrade the hardware of the VPN server at the factory in order to improve video quality in the camera streamings, so that the corresponding analysis can be performed later. The hardware upgrade was scheduled for Friday, as the server cannot be shut down during the week.","It was researched and confirmed that the VST Docker requires a Jetson device to function correctly, since there are specific libraries for those components.","An attempt was made to test the analysis of the streamings using external libraries (without using VST), but there were many compatibility errors, so it was decided to wait until a Jetson is available to use VST directly and follow the VST documentation ( https://docs.nvidia.com/jetson/jps/setup/quick-start.html)."]},{"l":"Activity Report - 08/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-4","l":"Main Updates","p":["To begin ruling out possible errors explaining why the camera streamings at Fimex appear poor when played via RTSP, a Python script was created that launches a local RTSP server using the Mediamtx software. A local 4K video downloaded from YouTube was used and streamed at different resolutions—from 144p up to 4K—to visually assess the smoothness and quality.","The script was also modified so that, in addition to streaming the videos at different qualities, it saves them locally in a folder organized by resolution. image","The videos were analyzed, and it was observed that lower-quality videos play more smoothly than higher-quality ones. This is likely due to the frame transmission: fewer frames result in faster transmission, while more frames slow it down slightly—but the differences are minimal.","An attempt will be made to adjust the Fimex streaming configurations to see if lowering the video quality improves streaming smoothness."]},{"l":"Activity Report - 09/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-5","l":"Main Updates","p":["The Python script was improved using the ffmpeg library to better re-stream local video via RTSP and to download videos in different qualities.","A method was researched to download videos directly from Hikvision software using its API.","A way to run Nvidia’s VST without using Jetson was researched and tested; configuration is ongoing, and further testing is needed to confirm its usability."]},{"l":"09/05/2025","p":["@VicmanGT","Fully implemented binary mask in the server surveillance","Detects movement of the camera based of the average difference in the pixel of two frames","If it's bigger than threshold then an alarm sounds","Also detects then an object obstructs the camera view, covering also that case","Some case that may affect funcitonality is when the camera is moved slowly the average diffence is small and therefore the alarm is not going to sound","https://github.com/user-attachments/assets/d587df69-1d0a-42c4-83e9-db9111e9a72e"]},{"l":"Activity Report - 12/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-6","l":"Main Updates","p":["An attempt was made to reinstall and configure the NVIDIA graphics drivers to enable the nvidia-smi command and allow the system to detect the GPU. However, it could not be fully configured, as the message \"Devices not found\" continued to appear.","A solution was sought to download streaming videos without losing quality. A Python script was created that saves the previous 5 minutes of the stream into short video clips. This method preserves both video quality and smooth playback.","Research was conducted on the protocols used by Hikvision's iVMS-4200 software. It was found that it uses a combination of protocols such as ISUP (Inter-System Unified Protocol), RTSP (Real-Time Streaming Protocol), and ONVIF protocol."]},{"l":"12/05/2025","p":["@VicmanGT","The webcam is working again","Continued investigation of error while trying to initialize pygame audio mixer","Couldn't yet find pages where the specific error was being solved","Added code to select pulseaudio specifically as the audio driver","And the error is different","To keep testing, code was added to ignore the audio in case is not available","Next thing is to add the funcionality of a visual alarm instead of a sound one"]},{"l":"Activity Report - 13/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-7","l":"Main Updates","p":["Another Python script was created to analyze and obtain the streaming videos from the 16 Fimex cameras, which resulted in 32 channels, as each camera offers a main channel with higher resolution and a secondary channel with lower resolution.","The purpose of this script is to locally save one minute of footage from each camera with a delay of 1 to 5 seconds from the live stream. This recorded video no longer has issues with fluidity or quality, but the downside is that it’s not entirely live.","It was detected that cameras 9, 10, and 11 on their main channel (channel 1) do not correctly record the full minute locally — they only save one literal second. However, the same cameras on their secondary channel (channel 2) do record the full minute correctly. The cause of this issue has not yet been identified.","An attempt was made to reinstall the drivers to get the GPU on the Zeus computer working properly again, in order to transfer the Python script there and run it as a service. However, the system still does not properly detect the graphics card."]},{"l":"Activity Report - 14/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-8","l":"Main Updates","p":["All drivers, including NVIDIA and CUDA, were completely reconfigured again on the Zeus computer to ensure proper functionality of the 5070 graphics card.","The Python script was improved to record every minute from the 16 camera streams of Fimex and save them locally on ZEUS's 14 TB hard drive.","A service was created to run every time the Zeus computer starts, so that at the 55th second of every minute, it executes the recording script in parallel for all Fimex cameras for later analysis.","Kevin was also assisted in setting up a VNC service with the MATE GUI for the Cronus computer. With this, we now have all three computers — Zeus, Gaia, and Cronus — equipped with a system for remote visualization."]},{"l":"Activity Report - 15/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-9","l":"Main Updates","p":["The Python script for recording Fimex camera streams was modified to remove the audio from all streams, and the organization for saving the videos was improved by structuring them by day and minute within their respective folders.","A new Docker container was configured using DeepStream together with NVIDIA VST to analyze the RTSP video streams from the cameras. However, the setup has not yet been able to complete the ROI and tripwire mapping on the streams, although an improvement in the smoothness and quality of each stream was observed.","An attempt will be made to run VST locally on the Zeus computer to analyze each saved video individually without relying on a specific NVIDIA Docker container, since there is a suspicion that it only works—or is better optimized—on Jetson devices."]},{"l":"15/05/2025","p":["@VicmanGT","Fixed audio driver problem by changing from pulseaudio to pipewire","Setup of virtual environment to have better control of library's version","Got past errors:","Fixed with the already found solution"]},{"i":"todo-1","l":"Todo:","p":["Check how to make the script run at all times","Implement robust error handling","Check what causes error message and if it affects:"]},{"l":"Activity Report - 16/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-10","l":"Main Updates","p":["Another attempt was made to configure VST for analyzing the video streams from some of the Fimex cameras, but it was unsuccessful. The method to draw ROI lines and tripwires in the system has not yet been figured out.","YOLOv5 was installed and configured on Zeus, along with the PyTorch libraries adapted for CUDA version 12.9.","Since VST has not been able to function properly, a local test was conducted using YOLOv5 and a new Python script designed to read all the videos recorded on a specific day for a specific camera, with the purpose of detecting people who appear or pass through each frame of the video. This was the first version of the script, but it still needs improvement.","I also provided support at Elite to restore functionality to a computer that suddenly failed, which was an urgent issue to resolve."]},{"l":"19/05/2025","p":["@VicmanGT","Fully implemented the alarm system for the Zeus server","Setted a service to make it run 24/7","Added logging statements to debug in case of errors","Commented lines of code from cv2 to prevent it to make calls to the GUI","There's need for physical testing to see if the alarm works"]},{"l":"Activity Report - 19/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-11","l":"Main Updates","p":["The Python algorithm for detecting people using YOLO was slightly improved, although progress on that was paused as the priority shifted to improving the streaming quality of the Fimex cameras.","To improve the quality of video downloads from the Fimex cameras, the algorithm was changed to enhance the frame rate and avoid encoding in a format that causes frame loss. However, we encountered an issue where the VPN server stopped working since the morning. I contacted Fernando (Fimex’s IT manager) to find a solution, but I haven’t received any response yet.","I also developed another Python algorithm that takes the saved one-minute video clips and generates an RTSP stream, so it can later be used in the VST, since it seems to only accept that format."]},{"l":"Activity Report - 20/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-12","l":"Main Updates","p":["I downloaded the code from the VST container in Envida to inspect it locally, file by file. After the inspection, I realized that there are binaries or executables, headers, .json configuration files, and libraries being used, but I couldn’t find actual source code that shows the full programming logic of the VST module.","I created an account on Wowza to be able to stream RTSP publicly from that platform in order to later test it with the VST. Although I managed to stream a video I downloaded from YouTube, for some reason the VST still didn’t detect it to display an image — even though both FFplay and VLC did.","I also made the Python algorithm stream previously recorded videos (before the Fimex VPN failed) over RTSP, so I could stream them locally from the VST. However, this also failed — it was not able to detect any image.","The issue with the VPN used to access the Fimex cameras’ streams still persists, so I couldn’t make any further progress in resolving the video transmission quality issue."]},{"l":"Activity Report - 27/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-13","l":"Main Updates","p":["The VNC UI was changed to the Mate UI on the ZEUS computer, as it had recently been replaced with a more basic one.","A test was performed with the office camera to connect it via RTSP to NVIDIA's VS; the video could be viewed, but unfortunately, the ROI and tripwire drawing have not yet been achieved.","Initial configurations and implementations of NVIDIA’s Video Search and Summarization Agent were carried out. The web UI was successfully launched and one of the videos was selected for analysis, although the database still needs to be configured due to some errors. image","Final modifications and bug fixes were made on Fidestech."]},{"l":"Activity Report - 30/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-14","l":"Main Updates","p":["Alternatives were researched to use NVIDIA's VST through libraries like DeepStream locally, as well as other visual interpretation libraries, since the Docker or container provided by NVIDIA is highly optimized for use on a Jetson device.","Additional configurations were made to NVIDIA's Video Search and Summarization to run it locally. Tests were conducted using a pre-recorded video from the Fimex camera we had stored and with the RTSP stream from the office camera at Nuclea. While we were able to retrieve the video feed, an error related to the database appeared when attempting to analyze or summarize the content.","Based on that error, a Python script was developed to simulate a service called NIM from NVIDIA, aiming to launch an embedding-based database on a specific local computer port. However, even with this workaround, it has not been possible to successfully analyze videos uploaded to the Video Search and Summarization UI.","In other projects, new versions were delivered and bugs were fixed on the Fidestech platform."]},{"l":"Activity Report - 02/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-15","l":"Main Updates","p":["A new solution was sought to run NVIDIA's Video Research and Summarization locally by removing some libraries and configuring new ports in order to run both the frontend and backend from the computer named Zeus. However, the expected result has not yet been achieved.","Recorded videos from NVIDIA's VST using the office camera via RTSP were analyzed and downloaded. It was discovered that the videos are indeed saved in the folder specified in the configuration file, stored in .mkv format within the previously set time intervals. They are also saved using the timestamp in milliseconds of the recording time, organized into folders named after the recording date.","Initial setup of the Jetson device at NUCLEA was started to perform a test using NVIDIA's VST, as the reason why the ROI and tripwire lines are not being traced in the recordings has not yet been identified."]},{"l":"Activity Report - 03/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-16","l":"Main Updates","p":["VNC was installed and configured on Nuclea's Jetson to remotely view the Mate UI from my local computer.","There were some issues getting browsers like Firefox or Chromium to work, but it was resolved by installing some missing Snap dependencies and configuring VNC to successfully run the Chromium browser on the Jetson.","The NVIDIA VST UI was successfully launched, and the initial configurations were completed from the Jetson. image","There were problems when trying to ping or view the office camera stream from the Jetson, as it is on a different network. An attempt was made to configure WireGuard to remotely connect to the camera's IP, but unfortunately, it was unsuccessful. Further tests will be attempted tomorrow or assistance will be requested during the meeting."]},{"l":"Activity Report - 04/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-17","l":"Main Updates","p":["Additional local tests were performed on Zesu's machine to get the backend with embeddings working for NVIDIA's Video Research and Summarization tool.","The necessary configurations were made to successfully launch the VST from the Jetson.","The office camera at Nuclea was successfully connected to the Jetson through VPN configurations applied in the WireGuard config file, allowing access to the camera's IP addresses.","The camera was successfully added to NVIDIA's VST, but when drawing the ROI and tripwire lines, the same rendering error appeared. An investigation will be conducted to determine which Jetson library is missing in order to fix this issue. image"]},{"l":"Activity Report - 10/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-18","l":"Main Updates","p":["A configuration of NVIDIA drivers was done along with the kernel to find one that satisfied the requirements of NVIDIA’s Video Search and Summarization.","The Video Search and Summarization documentation was followed step-by-step and in detail to run it in helmet mode, but in this mode we realized that a much more powerful GPU than the one we have is needed, as well as much more RAM. Specifically, the following error occurred: Detected NVIDIA GeForce RTX 5070 Ti GPU, which is not yet supported in this version of the container ERROR: No supported GPU(s) detected to run this container It was specified that the compatible GPUs range from H200 up to A100.","Seeing that helmet mode requires those configurations and that specific hardware, it was decided to opt for using Docker Compose to have a more customized configuration for our hardware. In this case, we used remote APIs like OpenAI for the LLMs and VLMs, and a Neo4j database which we managed to run correctly. More configurations are still needed in the Docker setup to run the VSS UI and be able to run it completely. I also think we will need an Azure OpenAI API key, which it seems to be using. image"]},{"l":"Activity Report - 11/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-19","l":"Main Updates","p":["Tests were conducted with Nvidia’s Video Search and Summarization (VSS) to try it using the resources of Zeus’s own computer. Lighter LLM and VLM models were used to allow analysis with our hardware, but due to library incompatibilities, the test could not be successfully completed.","The VSS Docker on Zeus’s computer was reconfigured from scratch to allow remote use of the LLM and VLM models, avoiding the full resource load on the machine. The OpenAI API key was used, and testing with Azure OpenAI is still pending. All backend modules loaded correctly, but an error occurs when launching the frontend, which I haven’t yet figured out how to fix in order to analyze video and RTSP camera streams."]},{"l":"Activity Report - 12/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-20","l":"Main Updates","p":["With the corresponding API keys to use the LLM and VLM models from OpenAI, and after updating libraries, drivers, and repositories for the graphics card of the computer Zeus, we were able to successfully run the UI of NVIDIA's Video Search and Summarization technology. Within it, we managed to link some of our pre-recorded videos from Fimex's cameras for analysis and subsequently generate a summary from them.","Unfortunately, even though the UI displays correctly and the backend is running properly, when making a query or asking something about the video, it gets stuck loading indefinitely and does not return any result, even though the console does not show any specific error. image","A new configuration will be attempted, using basic LLM and VLM models to be able to run everything locally and fully leverage the resources of the Zeus computer."]},{"l":"Activity Report - 13/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-21","l":"Main Updates","p":["Tests and library/model reconfigurations were carried out to better utilize NVIDIA’s Video Search and Summarization, since leaving all processing to be done remotely takes too long—even analyzing a simple 1-minute video segment becomes slow.","So, a combination of local and remote processing was implemented to significantly reduce load times. Unfortunately, the following error occurred in the Guardrails module, and I have not yet been able to find a solution: via-server-1 | 2025-06-14 02:50:19,117 ERROR Failed to load VIA stream handler - Guardrails failed via-server-1 | Traceback (most recent call last): via-server-1 | File /opt/nvidia/via/via-engine/via_server.py, line 1368, in run via-server-1 | self._stream_handler = ViaStreamHandler(self._args) via-server-1 | File /opt/nvidia/via/via-engine/via_stream_handler.py, line 409, in __init__ via-server-1 | self._create_llm_rails_pool() via-server-1 | File /opt/nvidia/via/via-engine/via_stream_handler.py, line 516, in _create_llm_rails_pool via-server-1 | raise Exception(Guardrails failed) via-server-1 | Exception: Guardrails failed via-server-1 | via-server-1 | During handling of the above exception, another exception occurred: via-server-1 | via-server-1 | Traceback (most recent call last): via-server-1 | File /opt/nvidia/via/via-engine/via_server.py, line 2880, in module via-server-1 | server.run() via-server-1 | File /opt/nvidia/via/via-engine/via_server.py, line 1370, in run via-server-1 | raise ViaException(fFailed to load VIA stream handler - {str(ex)}) via-server-1 | via_exception.ViaException: ViaException - code: InternalServerError message: Failed to load VIA stream handler - Guardrails failed via-server-1 | Killed process with PID 68"]},{"l":"Activity Report - 17/06/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-22","l":"Main Updates","p":["A meeting was held with Lalo to better understand why the SSL error was occurring when running VSS locally on the Zeus computer. We couldn't find the exact cause of the error, but we came up with the idea of running tests using NVIDIA's notebooks.","When testing on NVIDIA's notebooks, everything worked perfectly with the same configuration used on Zeus, so we continued analyzing other videos with summaries and questions in the chat.","Finally, I decided to reconfigure the NVIDIA Video Search and Summarization setup from scratch on the Gaia computer, and interestingly, it worked perfectly there. I was able to analyze individual MP4 videos and even the live stream from Nuclea's office camera. Although the summaries were generated for the camera stream, the video itself wasn’t displayed — but in the end, the NVIDIA VSS service was successfully executed. image"]}],[{"l":"27/03/2025","p":["Installed and configured Ubuntu 20.04 on local laptop with another ssd (dual boot, dual drive)","Got most of the requirements for Isaac Sim ( lacking VRAM (6.4 -> 8) and RAM (14.4 -> 32 ))","Installed Isaac Sim"]},{"l":"Todo:","p":["Start testing and exploring program"]},{"l":"28/03/2025","p":["Installed Isaac Lab","Made first tests","Quick start with robotic arm and solid cube","Got performance issues:","\"Isaac Sim is not responding\" multiple times","High RAM usage (300 Mb left)","OS frozen multiple times"]},{"i":"todo-1","l":"Todo:","p":["Check if there's a way to increase performance","Keep testing"]},{"l":"31/03/2025","p":["Installed Nvidia SDK Manager & Docker","Tests of tutorials in Isaac Sim","Worked well in GUI","Problems in interaction with python scripts","Message appearing when trying hot reload in vs code python script:","Last logs when trying to run sudo ./python.sh standalone_examples/api/isaacsim.simulation_app/hello_world.py","Not obvious reason atm"]},{"l":"1/04/2025","p":["Solved errors from yesterday","There was no clear reason at all","Most likely something went wrong during Isaac Lab instalation","Solution was to reinstall","Made other tutorials in nvidia page","Got performance errors in some of them","Likely cause: Limited VRAM memory in GPU"]},{"l":"Activity Report - 01/04/2025","p":["Email: brandon@nuclea.solutions"]},{"l":"Main Updates","p":["Watched all Multi-Camera Tracking tutorials from Nvidia: Nvidia On-Demand","Reinstalled Ubuntu 22.04, Isaac Sim, libraries, and drivers to prevent compatibility issues and ensure a clean work environment","Encountered issues during initial tracking tests in Isaac Sim, which led me to decide to reinstall everything.","Email: brandon@nuclea.solutions"]},{"i":"main-updates-1","l":"Main Updates","p":["An attempt was made to create a virtual machine in Google Cloud Console using Compute Engine, but the following service errors occurred:","An attempt was made to resolve the error by selecting different hosting zones and various server characteristics, but unfortunately, none were successful. image","Another test will be attempted with AWS to see if it works there."]},{"l":"2/04/2025","p":["@VicmanGT","Tested different included examples and tutorials in isaacsim packate","Got import errors in examples that tried to use clases defined in other folders","Exmaples that didnt' do that worked correctly","Got initialization error while trying to launch isaac-sim:","Got temporarly solved by rebooting Ubuntu","Not apparent cause yet.","Error seen in Nvidia Forum:","https://forums.developer.nvidia.com/t/cuda-error-999-failed-to-query-cuda-device-count-cuda-deviceordinal-is-invalid/274493","Email: brandon@nuclea.solutions"]},{"i":"main-updates-2","l":"Main Updates","p":["Successfully launch and configure a virtual machine on AWS without encountering server issues or GPU availability limitations by region.","The NVIDIA drivers with CUDA and other libraries were installed to configure Metropolitan NVIDIA.","An attempt was made to configure Metropolitan NVIDIA using Docker, but the following authorization error occurred:","I used DeepStream SDK as an alternative to Docker to perform intelligent video analysis.","I will look for a way to install it tomorrow using a Docker container for only the VTS service."]},{"l":"03/04/2025","p":["@VicmanGT","Check tutorials and examples from Nvidia Isaac Sim docs page","Started reviewing examples from cameras in the simulation","Printed frames into console and generated images from frames with opencv","Combined examples from a robot (car and arm ) simulation and a camera implementation, worked nicely"]},{"i":"todo-2","l":"Todo:","p":["Get video from simulation using camera","Check how to put multiple cameras and get data from them","Implement in other examples","Email: brandon@nuclea.solutions"]},{"i":"main-updates-3","l":"Main Updates","p":["It was successfully installed and configured DeepStream on the server.","A configuration file was created for DeepStream so that it could run a video through VST.","The video ran correctly, but I have the problem of not being able to visualize it since I'm connected via SSH.","Find a way to display the VST UI."]},{"l":"04/04/2025","p":["@VicmanGT","Checked humanoids example","Added multiple cameras to simulation in different positions and orientations","Got frames from all of them each a certain amount of time","Converted frames into iamges and store them in file system","Camera 1 first frame 1_camera1_opencv","Camera 2 first frame 1_camera2_opencv","Camera 3 first frame 1_camera3_opencv","Camera 1 second frame 2_camera1_opencv","Camera 2 second frame 2_camera2_opencv","Camera 13 second frame 2_camera3_opencv"]},{"i":"todo-3","l":"Todo:","p":["Modify humanoid movement to they don't fall that quick"]},{"l":"Activity Report - 07/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-4","l":"Main Updates","p":["The DeepStream configuration for running MP4 videos has been completed. It runs smoothly, although the interface has not yet been viewed.","The setup for running videos via the RTSP protocol in DeepStream has begun, but it is not yet ready due to the lack of a graphics engine with NVIDIA's TensorRT library, which is used for optimizing and running neural networks on GPUs. image","The plan is to follow the quick start guide for multi-camera simulation with AWS, from: Multi_Camera_Sim2Deploy_AWS"]},{"l":"07/04/2025","p":["@VicmanGT","Implemented 3 cameras in different position in humanoid simulation","Got 1 fps from all of them and were saved in different folders","Used numeric keyboard to move the humanoids throughout the space ( now modified to warehouse environment )","Simulation Results (5x vel):","https://github.com/user-attachments/assets/857a68e0-6a97-43a5-8a4b-a65aecdfccd5","Cameras frames:","https://github.com/user-attachments/assets/37780be7-548c-43ed-ba8e-8d2b6116535b","https://github.com/user-attachments/assets/055b8177-7941-450b-abac-d18915dcbd03","https://github.com/user-attachments/assets/f38c2686-e087-4d51-bd08-f0ae348baad8","Got some issues with slowness of the simulation and response time from the keyboard input, posible cause the frame capture","Neither RAM or VRAM seem an cause"]},{"l":"08/04/2025","p":["@VicmanGT","Implemented RF-DETR algorithm on code to make predicions on the detected frames for each camera","GitHub Repo: https://github.com/roboflow/rf-detr","Save the images with surrounding boxes with predictions","Got errors while trying this:","This after making a ./python.sh -m pip install rfdetr to install the library to use the model","The error message was showed in console for almost every omni dependent package","Ran ./post_install.sh after rebooting system","New error was this:","Neither the application of any of the examples from isaac sim worked due to the same error","Couldn't yet find a quick solution in the web"]},{"i":"todo-4","l":"Todo:","p":["Reinstall Isaac Sim from scratch","Check installed python libraries with ./python.sh -m pip list before and after trying to install the rfdeter"]},{"l":"Activity Report - 09/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-5","l":"Main Updates","p":["Early access was requested from NVIDIA through an official application to use their new AI models and camera tracking technologies, such as NVIDIA Metropolis. I am currently waiting for approval. image","Two work plans were developed to extract video footage from HikVision cameras. The first plan involves downloading videos locally via RTSP using the FFmpeg library. A Python script was created to manage all registered cameras listed in a CSV file. The recording duration is customizable, and using cron jobs and system services, we can schedule the script to run at specific times of the day. image","The second plan builds on the first but differs in that instead of storing the videos locally, the footage is uploaded directly to a cloud storage solution such as an AWS S3 bucket.","Tomorrow, we will begin testing with the actual Fimex cameras and evaluate the strategy of also using an SFTP server to store all videos, which will later be processed using NVIDIA Metropolis."]},{"l":"09/04/2025","p":["@VicmanGT","Reinstalled Isaac Sim and the problem from yesterdy solved, even after installiing de library for the RF-DETR","Correctly implemented RF-DETR algorithm","Correctly saved frames with bounding boxes of predicitons","Improved performance of simulation by lowering the resolution of the cameras (full hd -> hd)","Moved camera positions to get them to capture a wider space","Implemented an option to choose between automated simulation of manual control with the numpad","Started coding the algorithm to detect movement","Results of simulation:","https://github.com/user-attachments/assets/2f4ab757-2c00-4ba6-847a-94ed0731e8e0","Result frames with predicitons","https://github.com/user-attachments/assets/0ff68486-a04d-4fbf-9df4-60e08fe3807f","https://github.com/user-attachments/assets/85516f4c-dab2-4da7-a751-920e4efccdc5"]},{"i":"todo-5","l":"Todo:","p":["The RF-DETR algorithm had problems to identify the humanoids, labeling them as other objects when they were even detected","This could be mainly due to the not human like texture they have, so try to find if there's another texture to cover them","Check another object detection algorithm such as YOLO to compare performance and scores","Implement the move alert function","Figure out how to mantain some form of consistency during the frames passed"]},{"l":"Notes:","p":["Got this error message some times while trying to run the python script","The problem solved trying 2 or 1 more time, no obvious reason atm"]},{"l":"Activity Report - 10/04/2025","p":["Email: brandon@nuclea.solutions@VicmanGT"]},{"i":"main-updates-6","l":"Main Updates","p":["Today, I visited the Fimex factory and worked with Víctor on configuring and extracting video footage from the production cameras, then streaming it via RTSP to the AWS EC2.","Checked the stream via VLC, however this was only possible in Brandon's personal computer most likely because some specific configuration on the Local Fimex computer.","It did work one time on the Local Fimex computer but after changing the camera in the python script, the connection was unable to be setted again, the reason behind this it's still not clear.","We also left the local Fimex computer connected to TeamViewer, so we can access it remotely in the future for further configuration."]},{"l":"Activity Report - 11/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-7","l":"Main Updates","p":["A basic GUI was downloaded and configured for the AWS EC2 server, and a successful connection was made from a Windows client machine using VNC. image","A daemon service was created so that, upon starting or restarting the server, the mediamtx service would automatically start and run, enabling the reception of video from the RTSP cameras.","A VPN called Kerio was downloaded and configured on the virtual machine to establish a connection between the server and the local Fimex computer. image","A visit was made to Cumbres to perform a drone flight test."]},{"l":"11/04/2025","p":["@VicmanGT","Went to Cumbres school to help install and configure a sensor to better measure the distance from the ground","Helped performed basic flight operation"]},{"l":"Activity Report - 14/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-8","l":"Main Updates","p":["Once the VPN was correctly installed and configured on the EC2 server, a service was also created to enable it and connect automatically to the Fimex server every time the server starts.","A service was created so that VNC would start automatically when Linux boots, allowing us to access the graphical interface without having to activate it manually.","It was possible to ping the IP address 172.16.3.122, and the camera stream could already be viewed using the ffplay command. image image","It will be investigated how to save short video locally to process it on the EC2 server."]},{"l":"14/04/2025","p":["@VicmanGT","Coded a movement alert function that prints to the console when the centroid of a box prediction moves more that a certain threshold","Included logging info to know when the new object are detected, not longer detected or the classs of the prediction changed","Also when there are no predictions for the image","All this for each camera","https://github.com/user-attachments/assets/be6eac17-51a7-4254-bc91-5e1aee0add30","Implemented YOLOv11 model to make predictions on frames"]},{"i":"todo-6","l":"Todo:","p":["Check how to process the outputs of YOLOv11 to draw the rectangles on the image","Adapt function to process the output of YOLOv11","Email: brandon@nuclea.solutions"]},{"i":"main-updates-9","l":"Main Updates","p":["Some issues were fixed during the VPN initialization that were preventing it from properly connecting to the Fimex server.","After successfully configuring the VPN to access all camera streams from Fimex, a Python script was developed to extract specific segments of the live feed and save them locally on the computer. image image","The script will be improved to analyze each of the extracted videos for camera tracking. A more local solution will be implemented, without using NVIDIA Metropolis, since access to the platform has not yet been granted."]},{"l":"15/04/2025","p":["@VicmanGT","Created repo to have version control in isaac sim code","Completely implemented YOLOv11 algorithm to make predictions for the isaac sim simulation","Assigned camera 2 to this","Tests results:","The YOLO algorithm performed worse than the RFDE, since in most of the frames none of the humanoids were detected","Therefore no moment whatsoever","https://github.com/user-attachments/assets/e668101a-0287-4461-b6dc-8672fbc261bc","Tested movement alert function in local webcam to check with \"real person movement\" worked nicely","https://github.com/user-attachments/assets/419b049d-e30c-460f-8943-aa1d6a6a3602"]},{"i":"todo-7","l":"Todo:","p":["Apply filter for person class and counting"]},{"l":"16/04/2025","p":["@VicmanGT","Divided humanoid simulation file into modules for more comfortable development","Added named paratemer to script to select the model to use (rf-dert or yolo)","IsaacSim code stop working in local computer","Got same error as in the 8/04/2025 but the procedure didn't work now","Not custom humanoids simulation app or any of the examples are running:","Last logs before shooting down app:","Get frozen while trying to run then suddenly stops","IsaacSim App selector works ok"]},{"i":"todo-8","l":"Todo:","p":["Check RAM and VRAM usage while trying to run a script","Check other versions","Email: brandon@nuclea.solutions"]},{"i":"main-updates-10","l":"Main Updates","p":["Tests were conducted with the cameras to extract videos from Fimex.","A new VNC service was enabled to allow faster and more efficient access to the server.","A service was created to retry the VPN connection to prevent potential data leaks and to improve video retrieval from the cameras.","A solution will be explored to extract all videos from all cameras via streaming and store them in a bucket with a UI to view the streams simultaneously.","Email: brandon@nuclea.solutions"]},{"i":"main-updates-11","l":"Main Updates","p":["A Python script was created to display the streaming of the 20 cameras connected in Fimex through a simple UI. image","Another script was developed to continuously save the camera streams to an AWS S3 bucket. Whether the recording day ends or the script is interrupted due to an error, the recorded footage up to that point is saved automatically.","A Linux service was configured to ensure the script runs automatically at all times, without the need for manual startup.","The setup of a local computer with a 5070 graphics card will begin, aiming to eliminate the need for using AWS EC2 instances."]},{"l":"22/04/2025","p":["@VicmanGT","Tested custom models with the people counting function and the movement alert:","best.pt","https://github.com/user-attachments/assets/8ae3f7ba-64e9-4b74-9b0e-67b2b45cc418","NucleaDrone-v14-2Class.pt","https://github.com/user-attachments/assets/32f82b47-f2f7-4d12-9719-09a25f5dac72","Both models got similar results","Only minor difference is that best.pt is faster and therefore better and keeping track of people when do fast movements"]},{"l":"23/04/2025","p":["@VicmanGT","Cloned metropolis-dev repository into ec2 server to access compute there","Made predictions in videos of Fimex with rfde-tf, custom models best.pt and NucleaDrone-v14-2Class.pt","The rf-detr permormed well both at counting and the interaction with the movement_alert","https://github.com/user-attachments/assets/e4544a9c-d994-498d-8d65-57ae532acdf7","The tests made with both best.pt and NucleaDrone-v14-2Class.pt seemed to have less accuracy when detecting people, and only outputed one person in each frame"]},{"i":"todo-9","l":"Todo:","p":["Check what is happening there"]},{"l":"Activity Report - 24/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-12","l":"Main Updates","p":["The local Nuclea computer with an Nvidia 5070 graphics card was configured to access the Fimex cameras.","VPN and VNC services were created to enable access to the FSTP video from the cameras.","A script was created to run automatically and locally save the video stream from 20 cameras on the computer."]},{"l":"24/04/2025","p":["@VicmanGT","Corrected script to get better predictions from custom models best.pt and NucleaDrone-v14-2Class.pt","Tested with videos saved from Fimex cameras","best.pt","https://github.com/user-attachments/assets/4187d91b-af04-4b4d-a9da-73f096d703f7","NucleaDrone-v14-2Class.pt","https://github.com/user-attachments/assets/15da1964-4ead-43fd-9658-97365a215acb","Between the two custom models, best.pt got the better results being faster and more accurate","When comparing best.pt with the rf-dert model, both provide good results with the main difference that best.pt sometimes detect more people in the frame however rf-dert got more consistent and stable predictions."]},{"l":"Activity Report - 25/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-13","l":"Main Updates","p":["The advanced configuration was completed for the Zeus computer to enable remote access to its operating system's UI via VNC. image","The Python script was improved to more efficiently detect recordings or streamings from the cameras simultaneously and correctly save the video output.","The configuration of the Gaia computer is planned to be completed next, with the goal of enabling remote UI access and beginning the installation of software such as Isaac Sim and Isaac Lab."]},{"l":"25/04/2025","p":["@VicmanGT","Reestructured scripts in metropolis-dev repo into folders","isaac_sim_lab, models ( custom models ), utils ( movement alert and function to annotate yolo detections), and video_analysis","In video_analysis added separate scripts to test models in webcam and with a folder with videos"]},{"l":"Activity Report - 28/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-14","l":"Main Updates","p":["The new 14TB hard drive was configured and partitioned for the Zeus computer, and a shortcut was created for this drive in the file explorer.","The script for recording FIMEX cameras was modified to save all videos on the new hard drive.","All the initial configurations for the Gaia computer were completed to enable access to the operating system's GUI via a VNC service. Additionally, basic programs such as Visual Studio Code, VLC, Google Chrome, and others were installed. image"]},{"l":"28/04/2025","p":["@VicmanGT","Cloned local repository from metropolis-dev and adapted code in the Zeus computer for testing the movement alert in the recordings from Fimex","Got following errors:","Seemed like there isn't yet any Pytorch version that supports the installed GPU NVIDIA GeForce RTX 5070 Ti and therefore no models can be used","This happens for both custom models and rf-detr","Relevant links of same problem but using ComfyUI:","https://github.com/comfyanonymous/ComfyUI/issues/7127","https://github.com/comfyanonymous/ComfyUI/discussions/6643","Solved by installing another pytorch version:","Results:","Very similar to the ones made in the EC2 instance, however there were many videos that had a lot of noise that made the detections inaccurate and inconsistent","Example:","https://github.com/user-attachments/assets/5ffefcec-4474-4f50-bb0e-d34da1fe724d","But it seems like it's more of a problem of the cameras themselves."]},{"l":"Activity Report - 29/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-15","l":"Main Updates","p":["Isaac Sim and Isaac Lab from NVIDIA were downloaded and configured on the Gaia computer. image","The first test of using the VST container on the Zeus computer for elk tracking with the office webcam was conducted.","The VST Docker container runs correctly, but there is an issue where the UI does not display, even though no specific error is thrown. Further investigation is needed to find a way to launch it properly. image"]},{"l":"29/04/2025","p":["@VicmanGT","Started programming code for the Zeus server surveillance using webcam","Face detection algorithms with a Cascade Classifier might be useful","There are different types of configurations for the models managed in .xml files","There were issues when the camera pointed to the side of the face instead of in the front, and in this case there was no prediction","Relevant links:","https://www.geeksforgeeks.org/face-detection-using-cascade-classifier-using-opencv-python/","https://chatgpt.com/share/6811b087-52e0-800c-8bbc-b46e9086b737","https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html"]},{"i":"todo-10","l":"Todo:","p":["Find configuration file that can detect a face completely from the side","Check another way to detect when some is using the computer server","Pose Estimation, Fase Mesh, Eye tracking","Test"]},{"l":"Activity Report - 30/04/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-16","l":"Main Updates","p":["The Zeus computer was successfully configured to use NVIDIA NGC with access to all containers provided by NVIDIA Metropolis.","The first Docker container for NVIDIA VST was downloaded and used.","The UI was successfully displayed, although I still haven't found a way to connect the VST service to the local computer's webcam.","It is necessary to investigate how to run the webcam over RTSP and connect it directly to the Docker container so it can detect it, and also check if it would work with access to the camera recordings from Fimex."]},{"l":"30/04/2025","p":["@VicmanGT","Found repository that already had implemented a face detection algorithm and was able to detect face from the profile","Had some bugs but they're fixed now","Modifications:","Limit face detection to only the one that is closer to the camera by comparing the size of the bounding box","Added threshold to see if the face was close enough, simulating it's using the computer server","Filter by 'Right Profile' according to the accomodation of the webcam in the computer server so when it's detected, the code saved the frames and makes a video out of them","Relevant links:","https://github.com/nawafalageel/Side-Profile-Detection","https://www.geeksforgeeks.org/python-opencv-capture-video-from-camera/"]},{"i":"todo-11","l":"Todo:","p":["Check how to make multiple videos out of a single stream without the need to rerun the code"]},{"l":"Activity Report - 02/05/2025","p":["Email: brandon@nuclea.solutions"]},{"i":"main-updates-17","l":"Main Updates","p":["An RTSP service was configured so that the webcam from Nuclea's office could be accessed via RTSP streaming. image","A separate Docker container was used and configured to manage the VST more effectively.","The streaming from 4 Fimex cameras was successfully added directly to the VST UI for further analysis. image image","Investigate why the RTSP stream from the webcam did not work within the VST.","Check why I haven't been able to draw ROI zones and Tripwires on the Fimex streams that appear."]}],[{"l":"NVIDIA Isaac Sim & Isaac","p":["NVIDIA Isaac Sim is a high-fidelity robotics simulation application built on NVIDIA Omniverse. It is designed to help developers:","Design, simulate, test, and train AI-based robots in photorealistic virtual environments","Generate synthetic data for computer vision and sensor-based learning","Integrate with ROS/ROS 2 and other robotics middleware for real-world deployment","Isaac Sim leverages NVIDIA RTX ray tracing, advanced GPU-accelerated PhysX-based physics, and Universal Scene Description (USD) to provide a scalable, modular simulation platform. Its companion tools (such as Isaac Lab) are used for robot learning and reinforcement learning (RL) experiments."]},{"l":"HW Requirements","p":["GPU: RTX 3060 ( will sufice)","RAM: 32 GB ( may be need to upgrade )"]},{"l":"SF Requirements","p":["OS: Ubuntu 20.04 - 24.04 (need to check dual boot options to maximamize local computational power usage)","Docker ( maybe)","VS Code","GPU Drivers","Isaac Sim ( https://docs.isaacsim.omniverse.nvidia.com/4.5.0/installation/install_workstation.html)"]},{"l":"Guides","p":["https://www.nvidia.com/en-us/on-demand/playlist/playList-62b777fa-766f-4773-8ae4-a70e564d7848/"]}],[{"l":"NVIDIA Jetson","p":["La plataforma de desarrollo de Nvidia Jetson es una de las más populares para el desarrollo de aplicaciones embebidas de inteligencia artificail y robótica. Ofrece unidades de procesamiento compactas y potentes compatibles con el SDK JetPack de Nvidia que permite un desarrollo más rápido y eficiente.","Actualmente (22-04-2025) no se ha determinado si la plataforma Jetson será la plataforma para todos los desarrollos de covenant pero es una de las posibilidades más prometedoras.","Algunas de las funciones que Covenant planea implemntar en la plataforma Jetson son:","Control autónomo del dron: Cálculo de ruta y evasión de obstáculos","Video vigilancia: Monitoreo de situaciones de riesgo mediante el uso de modelos de visión artificial","Retransmisión de video: Enviar el video de la cámara FPV a un servidor para su consumo remoto","El modelo con el que se cuenta acutalmente es:","Jetson Orin Nano 8GB (Super Developer Kit) Jetpack 6.1 (Ubuntu 22.04)"]}],[{"l":"Configuraciones y herramientas","p":["Documentación sobre las diferentes herarmientas y configuraciones que se han utilizado para el desarrollo de los diferentes proyectos implementados sobre la plataforma de desarrollo NVIDIA Jetson"]},{"l":"YOLO","p":["Modelo de detección y segmentación de objetos enfocado en tiempo real, de la mano de Ultralytics.","Para instalar exitosamente la la libreria de Ultralytics siga el siguiente procedimiento:"]},{"l":"1. Actualizar los repositorios"},{"l":"4. Crear un entorno virtual (opcional)"},{"l":"3. Instalar la libreria de ultralytics","p":["Se pude omitir [export] si no se desea cambiar de formato los archivos de pesos","En versiones recientes de pip, el dependency resolver toma demasiado tiempo para resolver la dependencia adicional de export de ultralytics. Por lo que se recomienda agregar","--use-deprecated=legacy-resolver"]},{"l":"4. Instalar cusParselt","p":["Los comandos mencionados son para las versiones específicas que se encuentran en la jetson. Para posibles actualizaciones visitar el sitio oficial"]},{"l":"5. Instalar PyTorch y Torchvision","p":["Los whl son para la version 6.1 de Jetpack. Para posibles versiones visitar el sitio oficial de Nvidia o la guía de Ultralytics"]},{"l":"6. Instalar onnxruntime","p":["Onnx runtime cambia la versión de numpy por lo que se debe reinstalar una versión en específico"]},{"l":"Tensorrt","p":["Librería de NVIDIA para C++ que facilita inferencias de alto desempeño en unidades de procecsamiento graficas.TensorRT toma una red entrenada, compuesta por una definición y un grupo de pesas y produce un motor de inferencia optimizado","Existen múltiples formas de instalar tensorrt en Jetson. Pero con la que se tuvo éxito fue la siguiente:"]},{"l":"Jtop (Jetson Stats)","p":["Programa que muestra estadisticas de la Jetson en tiempo real, como el uso de GPU y que cuenta con interfaz programática en python","Jetson Stats"]},{"l":"Wadi","p":["Wadi es un servicio de streaming que utiliza el protocolo WebRTC para transmitir video en tiempo real a un servidor WHIP. Se utiliza para transmitir el video de la cámara FPV a un servidor para su consumo remoto"]},{"i":"1-clonar-el-repositorio--wadi","l":"1. Clonar el repositorio Wadi"},{"l":"2. Compilar el proyecto"},{"l":"3. Copiar el ejectuable a la carpeta de binarios"},{"l":"4. Crear servicios y regla de udev","p":["Cree los siguientes archivos en la ruta especificada. Estos son ejemplos y pueden cambiar de acuerdo a sus necesidades","El siguiente servicio se ejecuta en espacio de usuario, observe la ruta","Configuraciones y herramientas","Recuerde que necesita habilitar el servicio mediante"]},{"l":"ZED SDK","p":["El SDK de ZED es una herramienta de desarrollo que permite interactuar con la cámara ZED de manera programática. Se utiliza para capturar y procesar la información visual de la cámara ZED."]},{"l":"1. Instalar el SDK de ZED","p":["Descargar el SDK desde su página ZED SDK","De preferencia descargar las versiones que no sean Release Candidates (RC)","Se probó con la versión de Jetpack 6.1 (la version 6.2 da algunos errores de instalación)"]},{"l":"2. Instalar el controlador de monolink","p":["Despues de instalar el controlador de monolink se debe reiniciar la Jetson"]},{"l":"3. Conectar la cámara","p":["3.1. Conectar flex al puerto de cam1 de la Jetson","3.2. Conectar flex a la tarjeta monolink de ZED","3.3. Conectar el cable GMSL2 a la tarjeta monolink de ZED","3.4. Conectar el cable GMSL2 a la cámara ZED","3.5. Energizar la tarjeta monolink de ZED (12v-19v min 2w)","3.6. Energizar la Jetson","3.7. Ejecutar el visor ZED_Explorer en la Jetson (opcional)"]},{"l":"Wireguard","p":["Due to compatibility reasons, the version distributed by default of wireguard doesn't work when trying to up the interface defined by our conf file. To fix this we can use the following clone which works on:","Use this version but run the steps from the Official Wireguard Site."]},{"l":"Sierra WWAN EM7455 USB Modem","p":["Este módulo permite establecer una conexión de datos a redes 4G/3G/2G. Y en linux requiere de drivers distribuidos por sierra. Los pasos para instalar dichos drivers se detalla a continuación"]},{"l":"1. Verificar que se tengan las herramientas de compilación","p":["Se requiere un compilador de c como gc y make. Ambos se encuentran en la paqueteria build-essential"]},{"l":"2. Verificar que se tengan las cabeceras de Linux","p":["En Jetson se encuentran en /usr/src/linux-headers-5.15.148-tegra/3rdparty/canonical y por lo general se encuentran en la instalación por defecto"]},{"l":"3. Descargar los drivers de Sierra","p":["Se pueden descagar los drivers directo de la página oficial de Sierra aqui con la única desventaja que en Tegra parece faltar un par de módulos (cdc-wdm,cdc_mbim) por lo que se deben agregar a los módulos a compilar. El paquete con los módulos faltantesel siguiente comando en la carpeta donde se encuentran los archivos \"usb\""]},{"l":"5. Configurar el módulo","p":["En teoría el módulo debería configurarse automáticamente con los datos del proveedor del chip, sin embargo en pruebas esto no ocurría por lo que se debe configurar de manera manual mediante Network Manager.","A continuación se detallan los comandos para configurar el módulo con el proveedor telcel. Se deberá reconfigurar si se cambia de proveedor"]},{"l":"6. Visualizar el estado del módulo"},{"l":"Comandos útiles"}],[{"l":"OpenHD Hardware Requirements"},{"l":"1) Core Components"},{"l":"Air Unit (On Drone)","p":["Single Board Computer (SBC)","Raspberry Pi 4B","Raspberry Pi Compute Module 4 (CM4) + carrier board (recommended)","Supported camera (CSI or USB)","Supported WiFi adapter (USB)","Power supply / BEC (regulated 5 V with good current handling)","Micro-SD card (16–32 GB recommended)"]},{"l":"Ground Unit","p":["Laptop or","SBC (e.g. Raspberry Pi 4B)","Supported WiFi adapter (USB)","Display (monitor, goggles, etc.)","Optional: Antenna upgrades"]},{"l":"2) Supported / Recommended SBC Options"},{"l":"Air (Preferred → Alternative)","p":["Raspberry Pi CM4 + carrier board","Best performance","Small + lightweight","Raspberry Pi 4B","Budget-friendly","Good enough for most setups"]},{"l":"Ground","p":["Laptop (best performance)","Raspberry Pi 4B"]},{"l":"3) Camera Options","p":["Most stable option: Raspberry Pi Foundation CSI cameras HD → best latency + stability"]},{"l":"Recommended CSI Cameras","p":["Raspberry Pi Camera Module V1","Raspberry Pi Camera Module V2","Raspberry Pi HQ Camera (IMX477)","Arducam IMX Camera Modules (e.g., IMX519) — compatible clones that include authentication chip"]},{"l":"Notes","p":["Must support the Raspberry Pi camera interface + security chip","Latency + resolution depend on sensor + Pi performance"]},{"l":"Special / USB Camera Options","p":["Some USB cameras supported","USB thermal camera examples:","Hti-301","Infiray T2","USB = higher latency. Only recommended for special use cases."]},{"l":"HDMI Input (via HDMI-to-CSI)","p":["Some HDMI → CSI boards supported","Latency increases","Useful for external HDMI cameras"]},{"l":"4) WiFi Adapter Options","p":["Only specific WiFi chipsets work reliably (must support monitor mode and oversampling)."]},{"l":"Recommended Chipsets","p":["RTL8812AU","RTL8812BU","RTL8811AU","RTL8814AU","RTL8822EU"]},{"l":"Good Adapters","p":["Adapter","Notes","ASUS USB-AC56","Very popular, stable","ALFA AWUS036AC","Long-range option","Alfa AWUS036ACH","Alternative long-range","EDUP adapters with RTL8812AU","Budget","Generic RTL8812AU dongles","Can work; verify quality","Good power supply is critical"]},{"l":"5) Storage","p":["microSD card","16 GB minimum","32 GB recommended","CM4 models with eMMC also supported"]},{"l":"6) Antennas","p":["Standard dipoles work","For long range:","Directional antennas","High-gain antennas","Matching antennas → better RSSI / SNR"]},{"l":"7) Power / Wiring","p":["5 V regulated BEC required","Must supply:","SBC","WiFi USB device (may pull >1A)","Good connectors + wiring recommended"]},{"l":"OpenHD Performance Build (High-End)"},{"l":"1) Air Unit (On Drone)"},{"l":"SBC","p":["Raspberry Pi Compute Module 4 (CM4)","Recommended: 4–8 GB RAM, eMMC"]},{"l":"Carrier Board","p":["Any CM4 carrier board with:","CSI socket","USB port (for WiFi dongle)","Good power input","Examples:","Waveshare CM4 IO Board","BIGTREETECH CB1/CM4 board","ArduCam CM4 carrier"]},{"l":"Camera","p":["Raspberry Pi HQ Camera (IMX477)","Lens recommendation:","6–12 mm lenses (good FOV + sharpness)","Optional upgrade:","Arducam IMX519 or IMX462 (low-light)"]},{"l":"WiFi Adapter (Air)","p":["ASUS USB-AC56","Alternative upgrades","Alfa AWUS036AC / AWUS036ACH"]},{"l":"Power","p":["5V BEC, 3A minimum (continuous)","Recommended: 4–6A peak","Noise-filtered preferred"]},{"l":"Cooling","p":["Passive + optional micro-fan"]},{"l":"Storage","p":["32 GB microSD or CM4 eMMC"]},{"l":"2) Ground Unit"},{"l":"Device","p":["Laptop(recommended)","i5 / Ryzen 5 or better or","Raspberry Pi 4B"]},{"l":"WiFi Adapter (Ground)","p":["Alfa AWUS036AC / AWUS036ACH","Optional:","Second adapter for diversity reception"]},{"l":"Antennas","p":["Directional for long range:","Patch","Helical","Yagi","Omnis for short range","Dipoles or pagodas"]},{"l":"3) Antennas (Both Sides)"},{"l":"Air Unit","p":["Lightweight dipole (RHCP/LHCP pair)"]},{"l":"Ground Station","p":["Long-Range Pair","Directional (helical / patch / yagi)","Omnidirectional diversity"]},{"l":"4) Optional Upgrades"},{"l":"OSD / Telemetry","p":["MAVLink telemetry embedded into video"]},{"l":"Diversity Receiving","p":["Multiple WiFi adapters improves stability and distance"]},{"l":"DVR (Ground)","p":["Record HD feed to disk","Recommended: laptop → ffmpeg / OpenHD receiver"]},{"l":"5) Complete Shopping List","p":["5V BEC 3–6A","Air Antenna","Air SBC","Air WiFi","Alfa AWUS036AC / ACH","ASUS USB-AC56","Camera","Carrier","CM4","CM4 eMMC / 32GB SD","CM4 heatsink","Component","Cooling","Ground Antenna","Ground Device","Ground WiFi","Laptop","Lightweight dipole","Patch / Helical","Pi HQ (IMX477)","Power","Recommended","Storage","Waveshare / ArduCam"]},{"l":"6) Notes","p":["CSI camera → lowest latency","CM4 handles encoding more reliably than Pi 4B","High-gain antennas dramatically increase range","Long-range performance depends on:","Antenna choice","Fresnel zone clearance","Ground station placement","Use short cables + keep RF hardware away from ESC noise"]}],[{"l":"Proposito general","p":["Paginas que no sabemos donde meter"]}],[{"l":"Manuelo247"},{"l":"06/10/2025","p":["Inicie con el uso de la libreria Reall3dViewer para visualizar los escaneos de los vuelos en una pagina hosteada por nosotros mismos"]},{"l":"07/10/2025","p":["Primeras pruebas de carga de modelos, comprobacion y transformacion de archivos de las pruebas de vuelo. Los .ply tienen que ser generados como gaussian splatting o no se podra transformar al formato compatible."]},{"l":"08/10/2025","p":["Terra tiene soporte para 3dgs(3D Gaussian Splatting) pero unicamente cuando se hace fotogrametria, el vuelo fue exitoso y contiene datos de la rtk."]},{"l":"09/10/2025","p":["Primera visualizacion exitosa del modelo con la libreria, la version 'full' de la prueba del vuelo del dia 03/10/25 tiene varios bloques (Dividido gracias al tamaño del escaneo), estos pueden ser fusionados y visualizarse a distintas calidades."]},{"l":"10/10/2025","p":["Se preparo la pagina para visualizacion de varios modelos con seleccion para cambiar entre ellos. Se vio un limitante de Fetch al momento de descargar o subir archivos, se hicieron pruebas para solucionarlo pero no hubo exito. Se cree que puede deberse a una propia limitante del codigo y de la capacidad de la gpu de cada maquina."]}],[{"l":"Wireguard","p":["´´´sh sudo iptables -P FORWARD ACCEPT´´´"]}],[{"l":"Tinygrad en Covenant","p":["Investigación relacionada a tinygrad y cambios que se han requerido en el proyecto base"]},{"l":"¿Está listo para funcionar con yolo en la comma?","p":["No."]}],[{"l":"YOLO en Comma3X","p":["Página para alojar la documentación e investación de la utlización de un modelo de YOLO en el proyecto Comma3X de la empresa Comma.ai"]},{"l":"Reporte de Rendimiento del Modelo de Tinygrad en la Comma"},{"l":"Introducción","p":["Este reporte analiza el rendimiento de dos enfoques distintos para la ejecución de modelos de YOLO en la plataforma Comma. Se compararon:","Compilación JIT en Tinygrad: Se utilizó el repositorio de Tinygrad con la herramienta de compilación Just-In-Time (JIT) para optimizar la ejecución del modelo en tiempo real.","Compilación a Thneed: Se empleó el repositorio de OpenPilot con un proceso de compilación adaptado a la Comma, transformando el modelo en el formato Thneed para mejorar su eficiencia."]},{"l":"Compilación JIT en Tinygrad","p":["La compilación Just-In-Time (JIT) permite optimizar la ejecución del modelo en tiempo real, aplicando optimizaciones específicas según el contexto de uso. El codigo de ejemplo de su uso puede verse en el archivo yolov8_onnx_jit.py."]},{"l":"Funcionamiento en Tinygrad","p":["Captura de operaciones: Tinygrad usa TinyJit para almacenar operaciones en jit_cache en lugar de ejecutarlas inmediatamente.","Optimización: Se aplican técnicas como fusión de operaciones, eliminación de redundancias y planificación de memoria.","Compilación de kernels: Se generan kernels optimizados para la GPU utilizando compiladores especializados (por ejemplo, NVCC para CUDA).","Ejecución: Los kernels se ejecutan en la GPU, asegurando un alto rendimiento.","Reutilización: Se almacenan resultados previos en caché para evitar cálculos innecesarios."]},{"l":"Compilación Thneed en OpenPilot"},{"i":"introducción-1","l":"Introducción","p":["El script compile2_nuclea.py permite la conversión de modelos ONNX al formato Thneed, optimizando su ejecución en la plataforma Comma."]},{"l":"Proceso de Compilación","p":["Carga del Modelo:","Se obtiene el modelo ONNX desde una URL o un archivo local.","Generación del Plan de Ejecución (Schedule):","Se ejecuta el modelo con una imagen de entrada.","Se extrae un plan de ejecución optimizado, eliminando operaciones innecesarias.","Transformación a Formato Thneed:","Se convierte el plan de ejecución en un formato compatible con Thneed.","Se guarda el modelo compilado en un archivo.","Pruebas y Validación:","Se compara el modelo Thneed con el modelo ONNX para garantizar la consistencia de los resultados."]},{"l":"Funciones Clave","p":["get_schedule: Obtiene el plan de ejecución del modelo.","schedule_to_thneed: Transforma el plan en formato Thneed.","thneed_test_onnx: Valida la consistencia entre ONNX y Thneed."]},{"l":"Resultados de Rendimiento","p":["Se evaluaron ambos enfoques utilizando modelos de YOLO en formato ONNX con tres tamaños diferentes: mediano, pequeño y nano. Los tiempos de ejecución obtenidos fueron los siguientes:","Tamaño del Modelo","JIT en Tinygrad","Compilación Thneed","Medium","914 ms","2.1 s","Small","822 ms","951 ms","Nano","308 ms","355 ms"]},{"l":"Análisis de Rendimiento","p":["La ejecución con JIT en Tinygrad fue en promedio un 13% más rápida que la compilación con Thneed, sobre todo en modelos pequeños que no tienen mucho que optimizar.","En general, la ejecución con JIT en Tinygrad fue en promedio más rápida que la compilación con Thneed, aprovechando mejor la GPU en cargas de trabajo más grandes como puede notarse con el modelo Medium."]},{"l":"Conclusión","p":["Los resultados muestran que ambos enfoques son viables para la ejecución de modelos en la Comma, con tiempos de respuesta eficientes en los tres tamaños de YOLO probados. Sin embargo, la compilación JIT en Tinygrad aprovechó mejor la GPU, obteniendo un rendimiento significativamente superior en modelos más grandes en comparación con Thneed.","Esto sugiere que, en aplicaciones donde la latencia es crítica, Tinygrad con JIT puede ser una opción más adecuada."]}],[{"l":"Cool Projects","p":["Proyectos de terceros que son relevantes a Covenant y parecen interesantes","Quick video: Efficient video loading and context prefill for hour-long video understanding","exo: Run your own AI cluster at home with everyday devices."]}],[{"l":"Estación de aterrizaje","p":["La estación de aterrizaje es un proyecto de hardware desarrollado por covenant con el propósito de ofrecer una plataforma de aterrizaje, despegue y monitoreo de un quadricoptero"]},{"l":"Manual (WIP)","p":["Manual de usuario y técnico que servirá como referencia a futuro"]},{"l":"Inicio rápido"},{"l":"VPN","p":["Para conectarse de forma sencilla y remota a la estación, es necesario ser parte de la VPN ZeroTier de covenant.","Si no se es parte, comunicarse con el equipo de desarrollo."]},{"l":"QGroundControl","p":["QGroundControl es el sistem de control de tierra que se utiliza para comunicarse con el dron. Se puede desrcargar de su repositorio https://github.com/mavlink/qgroundcontrol/releases","Al ejecutarlo debería conectarse de forma automática al dron, de no hacerlo vea la sección de Troubleshooting"]},{"l":"Video","p":["Para ver una vista general del estado del dron sin descargar QGroundControl, puede acceder a esta liga Cherum Donde si habilita WEBGPU en su navegador podrá observar la cámara del dron si se encuentra activa","Para ver únicamente el video puede visitar Video Server"]},{"l":"Arquitectura","p":["Se plantea tener 4 componentes en nuestro sistema:","API: Administracion de usuarios, estaciones, etc.","Video: Almacenamiento de video y stream hacia clientes","App: Enfocado en los clientes y desde la cual van a poder administrar estaciones, ver alertas, etc.","Estacion: Frigate custom mas software de monitoreo y actualizaciones.","En la imagen a continuacion se puede observar a mas detalle la arquitectura:","Arquitectura de la Estacion V1"]}],[{"l":"It","p":["I # IT","@jeduardofr: Instalacion y configuraciones para la infraestructura de Nuclea, Covenant, Moca, ZigZag y We.page. La idea es crear redes distintas para las distintas empresas, siguiendo un poco mas la idea de ISO."]},{"l":"01 - 12 - 2025","p":["@jeduardofr y @fairbrook: Montar rack y deje infraestructura corriendo."]},{"l":"02 - 12 - 2025","p":["@jeduardofr: Dejar red de respaldo con Starlink como failover. Dejar servidores corriendo conectados a la red, montar access point en su lugar temporal"]},{"l":"03 - 12 - 2025","p":["@jeduardofr: Instalacion de tubería para el resto de la oficina, cableado hacia rack y mejoras de ponchado."]}],[{"l":"Stack de self-host","p":["wireguard: VPN","pangolin: Reverse proxy con tuneles","minio: Almacenamiento (como S3 de AWS)","beszel: Monitoreo","lxd Contenedores para desarrollo"]},{"l":"Programas utiles para conexiones de red","p":["nc (netcat): conexiones con TCP y UDP","iptables: firewall","tcpdump: inspeccionar trafico de redes"]},{"l":"Ejemplos de comandos","p":["Permitir trafico hacia puerto, proveniente de ip, con protocolo p(tcp, udp, etc.)","Nota: Este comando pondra la regla hasta arriba del listado, lo que significa que se procesara antes que el resto de reglas que se tengan definidas.","Hacer una conexion hacia host con ip y puerto","Escuchar por trafico en cierta interfaz de red hacia cierto puerto"]},{"l":"Compartir red local a traves de wireguard (distinta a la red del wireguard)","p":["Modificar configuracion de algunos de los peer que tiene acceso a la red que nos interesa compartir y agregar las siguientes lineas en el apartado de Interface:","Las interfaces se pueden consultar con ip addr.","Modificar la configuracion del servidor para el peer que publico la nueva red, lo que se tiene que hacer es agregar la red local al peer o la ip del equipo (para mayor control, sobre lo que se comparte) en AllowedIPs seperado por comas, por ejemplo se podria ver:","Recordar reiniciar el servicio con sudo systemctl restart wg-quick@wg0, modificar el wg0 con la interfaz correcta.","Modificar archivo del peer que quiere tener acceso a esta red y al igual solo agregar la direccion local del peer que la expuso, por ejemplo:","Recordar reiniciar la conexion de wireguard para que los cambies tomen efecto con sudo wg-quick down wg0 sudo wg-quick up wg0 y cambiar la interfaz por la correcta."]},{"l":"Ambientes de trabajo virtual (Isaacsim + VNC)"},{"l":"LXC","p":["LXC será nuestra herramienta de contenerización ya que nos permite tener un ambiente muy similar al que tendríamos con una máquina virtual, es decir, por defecto tenemos acceso a un usuario no root, systemd y los paquetes estandar de ubuntu server. Instalarlo es muy sencillo","Al instalar le pedirá un tamaño para el sistema de archivos internos, nosotros decidimos utilizar 200GB por instancia, planea acorde","Una guía más afondo para la instalación se puede encontrar aquí","Para lanzar la instancia base utilizamos la imagen ubuntu:22.04"]},{"l":"Lightdm","p":["Este display manager es el encargado de inicializar el servidor de Xorg que utilizará posteriormente VirtualGL para renderizar por hardware el ambiente gráfico","Si tiene instalado otro display manager es necesario desinstalarlo, por ejemplo"]},{"l":"Nvidia drivers","p":["Si bien, en teoría cualquier drivers de nvidia funcionan, encontramos que los drivers oficiales en su versión MIT son los que hacían funcionar a todo el setup","Tanto el host como el contenedor deberán contar con exactamente la misma versión de drivers para funcionar"]},{"l":"Virtual desktop","p":["Para permitir que todo funcione sin que se tenga un display físico conectado, es necesario crear un dispositivo virtual en el contenedor por lo que podemos reemplazar el contenido de /etc/X11/xorg.conf","Para mantener consistentes los permisos es necesario reconstruir la configuración de xorg"]},{"l":"Virtual GL","p":["Este software permite añadir soporte para renderización por hardare de las llamadas a OpenGL desde la VNC. Para instalarlo es más fácil siendo sudo","Una vez instalado es necesario configurarlo para que tenga acceso a las interfaces de la tarjeta gráfica","En nuestro caso no restringimos a los usuarios de utilizar los dispositivos de nvida a pesar de que esto implique peor seguridad ya que estamos dentro de una red privada y en contenedores pero cambie las opciones de acuerdo a sus necesidades"]},{"l":"Turbo VNC","p":["Se eligió este servidor de VNC ya que está diseñado específicamente para funcionar con VirtualGL. Al igual que con VirtualGL, estos pasos deberán seguirse con el usuario sudo","Puede utilizar cualquier window manager que desee, en nuestro caso utilizamo mate más por gusto que por otra razón","Para facilitar que el vnc siempre se encienda con el contenedor y demás amenidades, creamos el siguiente servicio en /etc/systemd/user/turbo-vnc@.service","Para habilitarlo y ejecutarlo utilice el siguiente commando desde el usuario deseado","Para acceder al vnc desde fuera del contenedor y host es necesario crear un proxy desde lxc","El cliente de VNC recomendado es remmina o el mismo Turbo VNC"]},{"l":"Isaac sim","p":["Ya que utilizamos PX4, decidimos utilizar Pegasus Simulator como bridge de comunicación con el simulador y la versión de isaac compatible es la 4.2","No fuimos capaces de hacer funcionar Isaac Sim directamente con nuestro entorno de VNC por lo que decidimos utilizar su función de streaming como medida temporal. Esperamos que con las próximas versiones estos problemas se solucionen o encontremos otra forma de hacer esto","Para descargar el cliente de webrtc de isaac sim ejecute los siguientes comandos","Finalmente para ejecutar el simulador podemos ejecutar","Y en otra terminal ya que se haya terminado de inicializar isaac sim ejecutamos el cliente de webrtc"]},{"l":"Tutorial para iniciar la simulación"},{"l":"Recursos utiles","p":["Beginners guide to traffic filtering with nftables","Differences between iptables and nftables explained"]}],[{"l":"Video Summarizaer","p":["pip install -r requirements.txt pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128","ngc config set ngc registry model download-version \"nvidia/vila_vision_language_model:1.5.3b\" --dest ./models"]}],[{"l":"Video Summarizaer","p":["pip install -r requirements.txt pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128","ngc config set ngc registry model download-version \"nvidia/vila_vision_language_model:1.5.3b\" --dest ./models"]}]]